{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install ftfy regex tqdm\n! pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-27T13:54:09.243499Z","iopub.execute_input":"2023-06-27T13:54:09.243849Z","iopub.status.idle":"2023-06-27T13:54:38.787653Z","shell.execute_reply.started":"2023-06-27T13:54:09.243818Z","shell.execute_reply":"2023-06-27T13:54:38.786418Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.5.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.6)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-4e6mvyvo\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-4e6mvyvo\n  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.5.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.64.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.28.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369410 sha256=01310c825d057c3e7e9494e7293c77fbbe0c67503db4748808bd19b4d55b4fee\n  Stored in directory: /tmp/pip-ephem-wheel-cache-csqat0o1/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport cv2\nimport torch.nn as nn\nimport numpy as np\nimport torch\nimport os\nimport clip\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nfrom pkg_resources import packaging\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom torch.autograd import Variable\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:54:38.790282Z","iopub.execute_input":"2023-06-27T13:54:38.790663Z","iopub.status.idle":"2023-06-27T13:54:41.422619Z","shell.execute_reply.started":"2023-06-27T13:54:38.790625Z","shell.execute_reply":"2023-06-27T13:54:41.421699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"clip.available_models()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:54:41.424199Z","iopub.execute_input":"2023-06-27T13:54:41.424840Z","iopub.status.idle":"2023-06-27T13:54:41.434264Z","shell.execute_reply.started":"2023-06-27T13:54:41.424789Z","shell.execute_reply":"2023-06-27T13:54:41.433253Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['RN50',\n 'RN101',\n 'RN50x4',\n 'RN50x16',\n 'RN50x64',\n 'ViT-B/32',\n 'ViT-B/16',\n 'ViT-L/14',\n 'ViT-L/14@336px']"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-L/14@336px\", device=device)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:54:41.437194Z","iopub.execute_input":"2023-06-27T13:54:41.437705Z","iopub.status.idle":"2023-06-27T13:55:51.598564Z","shell.execute_reply.started":"2023-06-27T13:54:41.437674Z","shell.execute_reply":"2023-06-27T13:55:51.597544Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 891M/891M [00:54<00:00, 17.3MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_path= \"/kaggle/input/vizwiz/Annotations/Annotations/train.json\"\nval_path= \"/kaggle/input/vizwiz/Annotations/Annotations/val.json\"\ntest_path = \"/kaggle/input/vizwiz/Annotations/Annotations/test.json\"\n\nwith open(train_path) as f:\n    train_data = json.load(f)\nwith open(val_path) as f:\n    val_data = json.load(f)\nwith open(test_path) as f:\n    test_data = json.load(f)\n\npath_train_images=\"/kaggle/input/vizwiz/train/train\"\npath_val_images=\"/kaggle/input/vizwiz/val/val\"\npath_test_images=\"/kaggle/input/vizwiz/test/test\"\n\ntrain=pd.DataFrame(train_data)\nval=pd.DataFrame(val_data)\ntest=pd.DataFrame(test_data)\n\nprint(train.shape)\nprint(val.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:55:51.600302Z","iopub.execute_input":"2023-06-27T13:55:51.600693Z","iopub.status.idle":"2023-06-27T13:55:52.373119Z","shell.execute_reply.started":"2023-06-27T13:55:51.600659Z","shell.execute_reply":"2023-06-27T13:55:52.371182Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(20523, 5)\n(4319, 5)\n(8000, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(train.iloc[:,[0,1,2,4]],train.iloc[:,3], test_size=0.05, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:55:52.374430Z","iopub.execute_input":"2023-06-27T13:55:52.374864Z","iopub.status.idle":"2023-06-27T13:55:52.393589Z","shell.execute_reply.started":"2023-06-27T13:55:52.374831Z","shell.execute_reply":"2023-06-27T13:55:52.392283Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(19496, 4)\n(1027, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"data1=np.array(X_train.iloc[:][['image','question']])\ndata2=torch.empty((data1.shape[0],1536), dtype=torch.float).cuda()\nprint(data1.shape)\nfor i in range(len(data1)):\n    with torch.no_grad():\n            text_features=model.encode_text(clip.tokenize(data1[i,1]).cuda()).cuda()\n            image = Image.open(path_train_images+'/'+data1[i,0])\n            image = preprocess(image).unsqueeze(0).cuda()\n            image_features=model.encode_image(image).cuda()\n            combined_features=torch.cat([image_features,text_features], dim=-1).cuda()\n    data2[i]=combined_features.cuda()\nprint(data2.shape)\ntorch.save(data2, \"/kaggle/working/combined_features.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-27T13:58:21.824732Z","iopub.execute_input":"2023-06-27T13:58:21.825204Z","iopub.status.idle":"2023-06-27T14:42:21.970996Z","shell.execute_reply.started":"2023-06-27T13:58:21.825168Z","shell.execute_reply":"2023-06-27T14:42:21.969962Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(19496, 2)\ntorch.Size([19496, 1536])\n","output_type":"stream"}]},{"cell_type":"code","source":"data1=np.array(np.array(val.iloc[:][['image','question']]))\ndata2=torch.empty((data1.shape[0],1536), dtype=torch.float).cuda()\nprint(data1.shape)\nfor i in range(len(data1)):\n    with torch.no_grad():\n            text_features=model.encode_text(clip.tokenize(data1[i,1]).cuda()).cuda()\n            image = Image.open(path_val_images+'/'+data1[i,0])\n            image = preprocess(image).unsqueeze(0).cuda()\n            image_features=model.encode_image(image).cuda()\n            combined_features=torch.cat([image_features,text_features], dim=-1).cuda()\n    data2[i]=combined_features.cuda()\nprint(data2.shape)\ntorch.save(data2, \"/kaggle/working/val_combined_features.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-27T14:42:21.972989Z","iopub.execute_input":"2023-06-27T14:42:21.973930Z","iopub.status.idle":"2023-06-27T14:52:10.747409Z","shell.execute_reply.started":"2023-06-27T14:42:21.973894Z","shell.execute_reply":"2023-06-27T14:52:10.746390Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(4319, 2)\ntorch.Size([4319, 1536])\n","output_type":"stream"}]},{"cell_type":"code","source":"data1=np.array(np.array(X_test.iloc[:][['image','question']]))\ndata2=torch.empty((data1.shape[0],1536), dtype=torch.float).cuda()\nprint(data1.shape)\nfor i in range(len(data1)):\n    with torch.no_grad():\n            text_features=model.encode_text(clip.tokenize(data1[i,1]).cuda()).cuda()\n            image = Image.open(path_train_images+'/'+data1[i,0])\n            image = preprocess(image).unsqueeze(0).cuda()\n            image_features=model.encode_image(image).cuda()\n            combined_features=torch.cat([image_features,text_features], dim=-1).cuda()\n    data2[i]=combined_features.cuda()\nprint(data2.shape)\ntorch.save(data2, \"/kaggle/working/test_combined_features.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-06-27T14:54:44.090258Z","iopub.execute_input":"2023-06-27T14:54:44.090629Z","iopub.status.idle":"2023-06-27T14:56:43.903430Z","shell.execute_reply.started":"2023-06-27T14:54:44.090598Z","shell.execute_reply":"2023-06-27T14:56:43.902467Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(1027, 2)\ntorch.Size([1027, 1536])\n","output_type":"stream"}]}]}